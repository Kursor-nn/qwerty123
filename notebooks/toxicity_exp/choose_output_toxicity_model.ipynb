{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4373e44d",
   "metadata": {},
   "source": [
    "### Данный проект предназначен исключительно для образовательных целей. Мы не поддерживаем и не одобряем использование токсичных комментариев или языка, который может причинить вред другим. \n",
    "### Пожалуйста, используйте полученные результаты ответственно."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6b46c43-b0e0-47af-b57e-d8e2a8d820eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q 'numpy<2' pandas scikit-learn torch transformers datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dddd119f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, BertTokenizer, BertForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "039cc3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/datasets/blackmoon/russian-language-toxic-comments\n",
    "df = pd.read_csv('../../data/labeled.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7bf39a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generated with cursor.ai + gpt-4o-mini\n",
    "toxic_sentences = [\n",
    "    \"Ты никогда не сможешь добиться успеха, потому что ты слишком глуп.\",\n",
    "    \"Все вокруг тебя только и делают, что смеются над твоими неудачами.\",\n",
    "    \"Ты не заслуживаешь любви и уважения, потому что ты никчемный человек.\",\n",
    "    \"Твои идеи всегда проваливаются, и никто не хочет с тобой работать.\",\n",
    "    \"Ты просто паразит, который живет за счет других людей.\",\n",
    "    \"Ты всегда будешь неудачником, и это не изменится.\",\n",
    "    \"Никто не хочет с тобой общаться, потому что ты слишком скучен.\",\n",
    "    \"Ты не способен на что-то великое, просто смирись с этим.\",\n",
    "    \"Твои попытки выглядеть умным только вызывают смех.\",\n",
    "    \"Ты никому не нужен, и это печальная правда, чмо.\"\n",
    "]\n",
    "\n",
    "positive_sentences = [\n",
    "    \"Ты действительно талантливый человек, и у тебя есть все шансы на успех.\",\n",
    "    \"Я восхищаюсь твоей работой и тем, как ты справляешься с трудностями.\",\n",
    "    \"Ты всегда поддерживаешь других, и это делает тебя особенным.\",\n",
    "    \"Твои идеи вдохновляют, и я уверен, что они принесут много пользы.\",\n",
    "    \"Ты обладаешь уникальными способностями, которые могут изменить мир к лучшему.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ccf655a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/y.korobko/Desktop/qwerty123/.venv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "toxicity_tokenizer = AutoTokenizer.from_pretrained('cointegrated/rubert-tiny-toxicity')\n",
    "toxicity_model = AutoModelForSequenceClassification.from_pretrained('cointegrated/rubert-tiny-toxicity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5e796754",
   "metadata": {},
   "outputs": [],
   "source": [
    "inappropriate_tokenizer = AutoTokenizer.from_pretrained(\"apanc/russian-inappropriate-messages\")\n",
    "inappropriate_model = AutoModelForSequenceClassification.from_pretrained(\"apanc/russian-inappropriate-messages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a3cfa0ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/Users/y.korobko/Desktop/qwerty123/.venv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "s_tokenizer = BertTokenizer.from_pretrained('SkolkovoInstitute/russian_toxicity_classifier')\n",
    "s_model = BertForSequenceClassification.from_pretrained('SkolkovoInstitute/russian_toxicity_classifier')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8185ac87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text2toxicity(text, aggregate=1):\n",
    "    \"\"\" Calculate toxicity of a text (if aggregate=True) or a vector of toxicity aspects (if aggregate=False)\"\"\"\n",
    "    with torch.no_grad():\n",
    "        inputs = toxicity_tokenizer(text, return_tensors='pt', truncation=True, padding=True).to(toxicity_model.device)\n",
    "        proba = torch.sigmoid(toxicity_model(**inputs).logits).cpu().numpy()\n",
    "    if isinstance(text, str):\n",
    "        proba = proba[0]\n",
    "    if aggregate:\n",
    "        return 1 - proba.T[0] * (1 - proba.T[-1])\n",
    "    return proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a00c67f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 ms ± 1.23 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "s_model(**s_tokenizer('Ужасный токсичный текст фу', return_tensors='pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "5932f5eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 ms ± 380 μs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "text2toxicity(\"Ужасный токсичный текст фу\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fdee2bfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35.1 ms ± 7.04 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "inappropriate_inputs = inappropriate_tokenizer(\"Ужасный токсичный текст фу\", return_tensors=\"pt\", padding=True, truncation=True)\n",
    "inappropriate_score = inappropriate_model(**inappropriate_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "39cb019a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learnable parameters in toxicity_model: 11.79 million\n",
      "Learnable parameters in inappropriate_model: 177.85 million\n",
      "Learnable parameters in s_model: 177.85 million\n"
     ]
    }
   ],
   "source": [
    "toxicity_model_params = sum(p.numel() for p in toxicity_model.parameters() if p.requires_grad)\n",
    "inappropriate_model_params = sum(p.numel() for p in inappropriate_model.parameters() if p.requires_grad)\n",
    "s_model_params = sum(p.numel() for p in s_model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Learnable parameters in toxicity_model: {toxicity_model_params / 1_000_000:.2f} million\")\n",
    "print(f\"Learnable parameters in inappropriate_model: {inappropriate_model_params / 1_000_000:.2f} million\")\n",
    "print(f\"Learnable parameters in s_model: {s_model_params / 1_000_000:.2f} million\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6b477581",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled Comments and Their Predicted Toxicity Scores:\n",
      "Comment: Машина постоянно у меня, в воскресенье аренда не списывается, аренда списывается со счета в ящере, по нему катаю только безнал, детский и комфорт, катаю его только 2т в день, остальное время катаю только диспечерские службы, Таксовик и 068,тариф меня устраивает, ну да где-то часов 12 в день, 4 часа ящер чтобы 2т накатать и 8 часов диспечерские 5-6т, 1500 бензин остальное моё\n",
      "\n",
      "Predicted Inappropriate: 0.03985416144132614\n",
      "Predicted Toxicity: 0.049443751828052385\n",
      "Predicted S-toxicity: 0.0020437585189938545\n",
      "Actual Toxicity: 0.0\n",
      "\n",
      "\n",
      "Comment: ой блять, как же вы заебали один еблан впишется в тему и прилично устроится и всё!1 - ОН, БЛЯТЬ, ВС СДЕЛАЛ САМ!1 И ВСЕГО ДОБИЛСЯ САМ!1 не по стечению обстоятельств, не потому что попёрло, а потому что ОН ВЕЛИКИЙ САМ!1 и тут же другие васи, которым чуть меньше в жизни подогнали и чуть хуже устроили, начинают ему подпевать - ВОТ СМОТРИТЕ! ОН САМ ВСЕГО ДОБИЛСЯ!1 ссука, какие же вы имбецилы... вы даже из мамкиной утробы сами хер появились, фанаты Золушки, блять\n",
      "\n",
      "Predicted Inappropriate: 0.9950690269470215\n",
      "Predicted Toxicity: 0.9999826367482536\n",
      "Predicted S-toxicity: 0.9965345859527588\n",
      "Actual Toxicity: 1.0\n",
      "\n",
      "\n",
      "Comment: Мне тебя жаль честно раз ты не можешь понять что там не написано что он был Прав и говоришь зачем поворачивать ещё двоих. Всё просто он убедился что он был не прав, пойми ты не прав был не ппрррааавввв по этому хватило двоих повернуть. Прочитай условия внимательно и этот текст сново условия и покажи кому нибудь из взрослых это чтоб объяснили тебе пробитому что ты сынок пробитый\n",
      "\n",
      "Predicted Inappropriate: 0.23449735343456268\n",
      "Predicted Toxicity: 0.7336184711050038\n",
      "Predicted S-toxicity: 0.8435376286506653\n",
      "Actual Toxicity: 1.0\n",
      "\n",
      "\n",
      "Comment: У меня жена была стюардессой от 40 до 100 получают, но здоровье быстро гробят\n",
      "\n",
      "Predicted Inappropriate: 0.01819128915667534\n",
      "Predicted Toxicity: 0.03165554767228329\n",
      "Predicted S-toxicity: 0.0013180709211155772\n",
      "Actual Toxicity: 0.0\n",
      "\n",
      "\n",
      "Comment: А про саму зп ничего нет)\n",
      "\n",
      "Predicted Inappropriate: 0.0011491511249914765\n",
      "Predicted Toxicity: 0.041785318298961815\n",
      "Predicted S-toxicity: 0.0008178619318641722\n",
      "Actual Toxicity: 0.0\n",
      "\n",
      "\n",
      "Comment: ты не у себя в школьном дотатреде и не на переменке, петушонок малолетний.\n",
      "\n",
      "Predicted Inappropriate: 0.9882092475891113\n",
      "Predicted Toxicity: 0.48733919468769926\n",
      "Predicted S-toxicity: 0.9348357915878296\n",
      "Actual Toxicity: 1.0\n",
      "\n",
      "\n",
      "Comment: Здесь прочитать можно(если немного увеличить)\n",
      "\n",
      "Predicted Inappropriate: 0.0014033099869266152\n",
      "Predicted Toxicity: 0.015058356771604053\n",
      "Predicted S-toxicity: 0.0008213295368477702\n",
      "Actual Toxicity: 0.0\n",
      "\n",
      "\n",
      "Comment: Хохол для битья, классика. И кто это говно смотрит вообще, вопрос хохлов пукин должен был решать, а не пиздоболы.\n",
      "\n",
      "Predicted Inappropriate: 0.9944936037063599\n",
      "Predicted Toxicity: 0.9998351407179933\n",
      "Predicted S-toxicity: 0.9928921461105347\n",
      "Actual Toxicity: 1.0\n",
      "\n",
      "\n",
      "Comment: судя по роже - не местный\n",
      "\n",
      "Predicted Inappropriate: 0.9465652704238892\n",
      "Predicted Toxicity: 0.6370271573487862\n",
      "Predicted S-toxicity: 0.003883229335770011\n",
      "Actual Toxicity: 1.0\n",
      "\n",
      "\n",
      "Comment: как и любой оператор каждый год\n",
      "\n",
      "Predicted Inappropriate: 0.0014143559383228421\n",
      "Predicted Toxicity: 0.02732244864135569\n",
      "Predicted S-toxicity: 0.00039785716217011213\n",
      "Actual Toxicity: 0.0\n",
      "\n",
      "\n",
      "Comment: Сдавали объект,надо испытания установки сделать и протоколы заполнить. Все хорошо, только лицензия лаборатории просрочена и поверки приборов. За пол-часа в paint поправил, все норм!\n",
      "\n",
      "Predicted Inappropriate: 0.0014576328685507178\n",
      "Predicted Toxicity: 0.10343516578071998\n",
      "Predicted S-toxicity: 0.0011663539335131645\n",
      "Actual Toxicity: 0.0\n",
      "\n",
      "\n",
      "Comment: Ты живёшь ради секса, долбоёб?\n",
      "\n",
      "Predicted Inappropriate: 0.994050920009613\n",
      "Predicted Toxicity: 0.9998402166888107\n",
      "Predicted S-toxicity: 0.994215190410614\n",
      "Actual Toxicity: 1.0\n",
      "\n",
      "\n",
      "Comment: После прочтения n-ного поста про коррупцию возникает идея ввести налог на коррупцию для чиновников. Типа вмененного дохода или приобретения годового патента.\n",
      "\n",
      "Predicted Inappropriate: 0.20483249425888062\n",
      "Predicted Toxicity: 0.32882034511722225\n",
      "Predicted S-toxicity: 0.005608055274933577\n",
      "Actual Toxicity: 0.0\n",
      "\n",
      "\n",
      "Comment: А не подтянулись бы, сказали бы что херовая Йота, даже на такие посты не реагирует.\n",
      "\n",
      "Predicted Inappropriate: 0.990575909614563\n",
      "Predicted Toxicity: 0.998641938277933\n",
      "Predicted S-toxicity: 0.9792799353599548\n",
      "Actual Toxicity: 1.0\n",
      "\n",
      "\n",
      "Comment: Первый оратор назвал Россию словом Рашка , а второй нашёл тупую причину доебаться до первого, мол Рашка это город в Сербии.\n",
      "\n",
      "Predicted Inappropriate: 0.9720253348350525\n",
      "Predicted Toxicity: 0.9997774370774155\n",
      "Predicted S-toxicity: 0.9750654697418213\n",
      "Actual Toxicity: 1.0\n",
      "\n",
      "\n",
      "Comment: А бандиты там как, понты любят?\n",
      "\n",
      "Predicted Inappropriate: 0.6630760431289673\n",
      "Predicted Toxicity: 0.6169941858459111\n",
      "Predicted S-toxicity: 0.023250142112374306\n",
      "Actual Toxicity: 1.0\n",
      "\n",
      "\n",
      "Comment: На органы его надо. На докторскую колбасу.\n",
      "\n",
      "Predicted Inappropriate: 0.9329739809036255\n",
      "Predicted Toxicity: 0.8359097914650828\n",
      "Predicted S-toxicity: 0.9803164601325989\n",
      "Actual Toxicity: 1.0\n",
      "\n",
      "\n",
      "Comment: Я ИНС-АЙДЫР, МАМОЙ КЛЯНУС! ВСЕ ВС ЗНАЮТ. Да, я об этом\n",
      "\n",
      "Predicted Inappropriate: 0.5249747633934021\n",
      "Predicted Toxicity: 0.02332381564344488\n",
      "Predicted S-toxicity: 0.9866955280303955\n",
      "Actual Toxicity: 1.0\n",
      "\n",
      "\n",
      "Comment: Может хоть 3, хоть 4 стороны расписываться. И не факт, что в одном городе\n",
      "\n",
      "Predicted Inappropriate: 0.017299706116318703\n",
      "Predicted Toxicity: 0.018728507773526637\n",
      "Predicted S-toxicity: 0.001274102833122015\n",
      "Actual Toxicity: 0.0\n",
      "\n",
      "\n",
      "Comment: Так клиент заплатил. ТС же ждал конца предоплаченого периода. А ТС задавал какие-то вопросы по работе. И ему не отвечали. Какие вопросы - он не уточняет. Поэтому весьма однобокий взгляд на ситуацию. Так же странно выглядит то, что ТС подсчитал прибыль у стоматолога. Как он учитывал расходы... )\n",
      "\n",
      "Predicted Inappropriate: 0.004527445416897535\n",
      "Predicted Toxicity: 0.054451135654207894\n",
      "Predicted S-toxicity: 0.003531079273670912\n",
      "Actual Toxicity: 0.0\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sampled_df = df.sample(n=20, random_state=21)\n",
    "inputs = inappropriate_tokenizer(sampled_df['comment'].tolist(), padding=True, truncation=True, return_tensors=\"pt\")\n",
    "inappropriate_scores = inappropriate_model(**inputs).logits.softmax(dim=1)[:, 1]\n",
    "toxicity_scores = [text2toxicity(sentence) for sentence in sampled_df['comment'].tolist()]\n",
    "s_inputs = s_tokenizer(sampled_df['comment'].tolist(), padding=True, truncation=True, return_tensors=\"pt\")\n",
    "s_scores = s_model(**s_inputs).logits.softmax(dim=1)[:, 1]\n",
    "\n",
    "sampled_df['predicted_inappropriate'] = inappropriate_scores.tolist()\n",
    "sampled_df['predicted_toxicity'] = toxicity_scores\n",
    "sampled_df['predicted_s_toxicity'] = s_scores.detach().cpu().numpy().tolist()\n",
    "\n",
    "\n",
    "print(\"Sampled Comments and Their Predicted Toxicity Scores:\")\n",
    "for index, row in sampled_df.iterrows():\n",
    "    print(f\"Comment: {row['comment']}\") \n",
    "    print(f\"Predicted Inappropriate: {row['predicted_inappropriate']}\") \n",
    "    print(f\"Predicted Toxicity: {row['predicted_toxicity']}\")\n",
    "    print(f\"Predicted S-toxicity: {row['predicted_s_toxicity']}\")\n",
    "    print(f\"Actual Toxicity: {row['toxic']}\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a181d332",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_message(message, max_length=None, print_message=True):\n",
    "    toxicity_score = text2toxicity(message)\n",
    "    inappropriate_inputs = inappropriate_tokenizer(message, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    inappropriate_score = inappropriate_model(**inappropriate_inputs).logits.softmax(dim=1)[0, 1]\n",
    "    s_inputs = s_tokenizer(message, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    s_score = s_model(**s_inputs).logits.softmax(dim=1)[0, 1]\n",
    "    max_length = max_length or len(message)\n",
    "    alignment_length = max_length - len(message)\n",
    "    if print_message:\n",
    "        print(f\"{message} {' ' * alignment_length} | Inappropriate: {inappropriate_score.item():.4f} | Toxicity: {toxicity_score:.4f} | S-toxicity: {s_score.item():.4f}\")\n",
    "    else:\n",
    "        print(f\"Inappropriate: {inappropriate_score.item():.4f} | Toxicity: {toxicity_score:.4f} | S-toxicity: {s_score.item():.4f}\")\n",
    "    # return toxicity_score, inappropriate_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae4e115d",
   "metadata": {},
   "source": [
    "В примере снизу обе модели плохо работают на некоторых токсичных предложениях"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e3cd9a68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toxic Sentences:\n",
      "Ты никогда не сможешь добиться успеха, потому что ты слишком глуп.                | Inappropriate: 0.4725 | Toxicity: 0.0662 | S-toxicity: 0.4042\n",
      "Все вокруг тебя только и делают, что смеются над твоими неудачами.                | Inappropriate: 0.0398 | Toxicity: 0.2950 | S-toxicity: 0.7879\n",
      "Ты не заслуживаешь любви и уважения, потому что ты никчемный человек.             | Inappropriate: 0.2449 | Toxicity: 0.0187 | S-toxicity: 0.7088\n",
      "Твои идеи всегда проваливаются, и никто не хочет с тобой работать.                | Inappropriate: 0.0875 | Toxicity: 0.0158 | S-toxicity: 0.1067\n",
      "Ты просто паразит, который живет за счет других людей.                            | Inappropriate: 0.8533 | Toxicity: 0.1021 | S-toxicity: 0.9586\n",
      "Ты всегда будешь неудачником, и это не изменится.                                 | Inappropriate: 0.3290 | Toxicity: 0.1335 | S-toxicity: 0.9129\n",
      "Никто не хочет с тобой общаться, потому что ты слишком скучен.                    | Inappropriate: 0.0201 | Toxicity: 0.4170 | S-toxicity: 0.8262\n",
      "Ты не способен на что-то великое, просто смирись с этим.                          | Inappropriate: 0.0221 | Toxicity: 0.0398 | S-toxicity: 0.1799\n",
      "Твои попытки выглядеть умным только вызывают смех.                                | Inappropriate: 0.0527 | Toxicity: 0.0354 | S-toxicity: 0.7328\n",
      "Ты никому не нужен, и это печальная правда, чмо.                                  | Inappropriate: 0.9885 | Toxicity: 0.9987 | S-toxicity: 0.9872\n",
      "\n",
      "Positive Sentences:\n",
      "Ты действительно талантливый человек, и у тебя есть все шансы на успех.           | Inappropriate: 0.0018 | Toxicity: 0.0174 | S-toxicity: 0.0136\n",
      "Я восхищаюсь твоей работой и тем, как ты справляешься с трудностями.              | Inappropriate: 0.0013 | Toxicity: 0.0153 | S-toxicity: 0.0148\n",
      "Ты всегда поддерживаешь других, и это делает тебя особенным.                      | Inappropriate: 0.0031 | Toxicity: 0.0286 | S-toxicity: 0.1918\n",
      "Твои идеи вдохновляют, и я уверен, что они принесут много пользы.                 | Inappropriate: 0.0015 | Toxicity: 0.0291 | S-toxicity: 0.0068\n",
      "Ты обладаешь уникальными способностями, которые могут изменить мир к лучшему.     | Inappropriate: 0.0063 | Toxicity: 0.1021 | S-toxicity: 0.0065\n"
     ]
    }
   ],
   "source": [
    "print(\"Toxic Sentences:\")\n",
    "for sentence in toxic_sentences:\n",
    "    test_message(sentence, max_length=80)\n",
    "\n",
    "print(\"\\nPositive Sentences:\")\n",
    "for sentence in positive_sentences:\n",
    "    test_message(sentence, max_length=80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fdf59f9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Вы должны грабить магазины  | Inappropriate: 0.9665 | Toxicity: 0.5296 | S-toxicity: 0.0060\n"
     ]
    }
   ],
   "source": [
    "test_message(\"Вы должны грабить магазины\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "740ad86c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Вы должны были грабить магазины  | Inappropriate: 0.4884 | Toxicity: 0.6299 | S-toxicity: 0.0054\n"
     ]
    }
   ],
   "source": [
    "test_message(\"Вы должны были грабить магазины\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "454845e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Вы должны будете грабить магазины  | Inappropriate: 0.9577 | Toxicity: 0.5582 | S-toxicity: 0.0092\n"
     ]
    }
   ],
   "source": [
    "test_message(\"Вы должны будете грабить магазины\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5a724dac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inappropriate: 0.0228 | Toxicity: 0.0462 | S-toxicity: 0.0132\n"
     ]
    }
   ],
   "source": [
    "test_message(\n",
    "    \"Грабить магазины это нормально это \"\n",
    "    \"нормально это нормально это нормально \"\n",
    "    \"это нормально это нормально это нормально \"\n",
    "    \"это нормально это нормально это нормально \"\n",
    "    \"это нормально это нормально это нормально \"\n",
    "    \"это нормально это нормально это нормально \"\n",
    "    \"это нормально это нормально это нормально \"\n",
    "    \"это нормально это нормально это нормально \"\n",
    "    \"это нормально это нормально это нормально \"\n",
    "    \"это нормально это нормально это нормально \"\n",
    "    \"это нормально это нормально это нормально \"\n",
    "    \"это нормально это нормально это нормально\", \n",
    "    print_message=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c90af8",
   "metadata": {},
   "source": [
    "Возможно позволительно закрыть глаза на такие ошибки, потому что обычный alignment не должен пропустить такие предложения."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
